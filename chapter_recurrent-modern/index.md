# 现代反复神经网络
:label:`chap_modern_rnn`

我们介绍了 RNN 的基础知识，它可以更好地处理序列数据。为了演示，我们在文本数据上实施了基于 RNN 的语言模型。但是，当从业人员现在面临着各种序列学习问题时，这些技术可能不足以使用他们。

例如，实践中一个值得注意的问题是 RNN 的数量不稳定性。尽管我们已经应用了梯度剪切等实现技巧，但通过更复杂的序列模型设计，这个问题可以进一步缓解。具体来说，门控 RNN 在实践中更常见。我们将首先引入两个这样广泛使用的网络，即 * 门控循环单元 * (GRU) 和 * 长短期内存 * (LSTM)。此外，我们将使用迄今为止讨论的单个无向隐藏层来扩展 RNN 架构。我们将描述具有多个隐藏层的深层架构，并讨论双向设计与前向和向后重复计算。现代经常性网络经常采用这种扩张。在解释这些 RNN 变体时，我们将继续考虑 :numref:`chap_rnn` 中引入的语言建模问题。

事实上，语言建模只能揭示序列学习能够实现的一小部分。在自动语音识别、文本转语音和机器翻译等各种序列学习问题中，输入和输出都是任意长度的序列。为了解释如何适应这种类型的数据，我们将以机器翻译为例，并介绍基于 rNN 和束搜索的编码器解码器架构，以便生成序列。

```toc
:maxdepth: 2

gru
lstm
deep-rnn
bi-rnn
machine-translation-and-dataset
encoder-decoder
seq2seq
beam-search
```
