

<!--
 * @version:
 * @Author:  StevenJokes https://github.com/StevenJokes
 * @Date: 2020-07-03 18:46:26
 * @LastEditors:  StevenJokes https://github.com/StevenJokes
 * @LastEditTime: 2020-07-03 20:52:02
 * @Description:translate
 * @TODO::
 * @Reference:http://preview.d2l.ai/d2l-en/PR-1102/chapter_optimization/lr-scheduler.html
-->

# 把学习率列入程序

到目前为止，我们主要关注如何更新权向量的优化算法，而不是它们被更新的速度。尽管如此，调整学习率通常和实际算法一样重要。有许多方面需要考虑：

- 最明显的是，学习率的重要性。如果它太大，优化发散，如果它太小，训练时间太长或者我们最终得到一个次优的结果。我们在前面看到，问题的条件号很重要(详见11.6节)。直观上，它是最不灵敏方向与最灵敏方向的变化量之比。
- 其次，衰变速率也同样重要。如果学习率仍然很大，我们可能会简单地在最小值附近跳跃，从而无法达到最优。第11.5节对此进行了详细讨论，并在第11.4节中分析了性能保证。简而言之，我们希望衰减速率，但可能比O(t12)O(t12)要慢，这对于凸问题来说是个不错的选择。
- 另一个同样重要的方面是初始化。这既与参数最初如何设置有关(详细信息请参阅4.8节)，也与它们最初如何发展有关。这被称为预热，也就是。，我们一开始走向解决方案的速度有多快。开始时的大步骤可能没有好处，特别是因为初始参数集是随机的。最初的更新指南也可能毫无意义。
- 最后，还有一些执行周期性学习率调整的优化变量。这超出了本章的范围。我们建议读者查阅[Izmailov等人，2018](http://preview.d2l.ai/d2l-en/PR-1102/chapter_references/zreferences.html#izmailov-podoprikhin-garipov-ea-2018)中的细节，例如，如何通过对整个路径的参数进行平均来获得更好的解决方案。

考虑到管理学习率需要很多细节，大多数深度学习框架都有自动处理这些问题的工具。在当前的章节中，我们将回顾不同的时间表对准确性的影响，也展示如何能通过一个学习率调度器有效地管理。

## 玩具问题

我们从一个玩具问题开始，它足够便宜，可以很容易地计算，但又足够重要，可以说明一些关键方面。为此，我们选择了略微现代化的LeNet版本(relu而不是sigmoid激活，MaxPooling而不是AveragePooling)，就像适用于Fashion-MNIST一样。此外，为了提高性能，我们对网络进行了杂交。由于大多数代码都是标准的，我们只介绍基本的内容而没有进一步的详细讨论。如有需要，请参阅[第6节](http://preview.d2l.ai/d2l-en/PR-1102/chapter_convolutional-neural-networks/index.html#chap-cnn)进行复习。

TODO:CODE

让我们看看如果我们使用默认设置调用这个算法会发生什么，比如学习率为 0.3 并训练 30 次迭代。请注意，训练精度是如何持续提高的，而测试精度的进展却超过了一个点。两条曲线之间的差距表示过拟合。

## 调度器

调整学习率的一种方法是在每个步骤中明确设置它。 这可以通过`set_learning_rate`方法方便地实现。 我们可以在每个时期（甚至在每个小批量生产之后）向下调整它，例如：以动态方式响应优化的进行情况。

TODO:CODE

更一般地，我们希望定义一个调度程序。当以更新次数调用时，它返回学习速率的适当值。让我们定义一个简单的函数，将学习速率设置为 η=η0(t+1)−12 .

TODO:CODE

让我们画出它在一系列值上的行为。

TODO:CODE

现在让我们看看这对Fashion-MNIST的训练有什么影响。我们只是将调度程序作为训练算法的附加参数提供。

TODO:CODE

这比以前好多了。有两点值得注意:曲线比以前更加平滑。其次，过度拟合较少。不幸的是，在理论上，为什么某些策略会导致不那么过拟合的问题还没有得到很好的解决。有一些争论认为，更小的步长会导致参数更接近于零，从而更简单。然而，这并不能完全解释这种现象，因为我们并不是真的早早地停止学习，而只是轻轻地降低了学习速度。

TODO:CODE


这个分段常数学习率计划背后的直觉是，让优化进行，直到一个平稳点已经达到权向量的分布。然后(也只有在那时)我们才能降低速率，例如获得一个高质量的代理到一个好的局部最小值。下面的示例展示了这如何生成更好的解决方案。
这比以前好多了。有两点值得注意:曲线比以前更加平滑。其次，过度拟合较少。不幸的是，在理论上，为什么某些策略会导致不那么过拟合的问题还没有得到很好的解决。有一些争论认为，更小的步长会导致参数更接近于零，从而更简单。然而，这并不能完全解释这种现象，因为我们并不是真的早早地停止学习，而只是轻轻地降低了学习速度。
## 单因素调度器
多项式衰减的一种替代方法是乘法，即α∈（0,1）的ηt+ 1←ηt⋅α。 为了防止学习率衰减到合理的下限之外，经常将更新方程修改为ηt+ 1←max（ηmin，ηt⋅α）。


TODO:CODE

这也可以通过MXNet中的内置调度器`lr_scheduler.FactorScheduler`对象。它需要一些更多的参数，如预热周期，预热模式(线性或常量)，期望更新的最大数量，等等;接下来，我们将在适当的情况下使用内置调度程序，只在这里解释它们的功能。如前所述，如果需要，构建自己的调度器是相当简单的。

### 多因素调度器

虽然我们不可能涵盖所有种类的学习率调度器，但我们尝试在下面简要介绍一些流行的策略。常见的选择是多项式衰减和分段常数调度。除此之外，余弦学习速率调度在一些问题上有很好的经验。最后，在一些问题上，在使用大学习率之前预热优化器是有益的。
训练深度网络的常见策略是保持学习率分段恒定，并每隔一段时间将其降低给定数量。 也就是说，给定一组降低速率的时间，例如s = {5,10,20}每当t∈s时就降低ηt+ 1←ηt⋅α。 假设每个步骤的值减半，我们可以按以下方式实现。

TODO:CODE

这个分段常数学习率计划背后的直觉是，让优化进行，直到一个平稳点已经达到权向量的分布。然后(也只有在那时)我们才能降低速率，例如获得一个高质量的代理到一个好的局部最小值。下面的示例展示了这如何生成更好的解决方案。

TODO:CODE

### Cosine调度器

[Loshchilov & Hutter, 2016](http://preview.d2l.ai/d2l-en/PR-1102/chapter_references/zreferences.html#loshchilov-hutter-2016)提出了一个相当令人困惑的启发式。它依赖于我们可能不想在一开始就大幅降低学习率的观察，而且，我们可能希望在最后使用一个非常小的学习率来改进解决方案。这导致了一个类似余弦的调度，具有以下函数形式，用于在t [0, t]范围内的学习率。

在这里，初始学习率是初始学习率，初始学习率是时间TT的目标率。此外，对于 t>T ，我们只需将该值固定到“继续”，而无需再次增加它。在下面的示例中，我们设置最大更新步骤 T=20 。

TODO:CODE

在计算机视觉的背景下，这个时间表可以导致改进的结果。但是请注意，这样的改进是不能保证的(如下所示)。

TODO:CODE

### 热身期

在某些情况下，初始化参数并不足以保证一个好的解决方案。这对于一些高级网络设计来说是一个特别的问题，可能会导致不稳定的优化问题。我们可以通过选择一个足够小的学习率来解决这个问题，以在开始时防止分歧。不幸的是，这意味着进展缓慢。相反，高学习率最初会导致分歧。
对于这种困境，一个相当简单的解决方法是使用一段热身期，在此期间学习率增加到最初的最大值，然后冷却学习率，直到优化过程结束。为了简单起见，我们通常为此使用线性增长。这将导致下面所示的表单的时间表。

TODO:CODE

注意，网络最初的收敛性更好(特别是观察前5个时期的性能)。

TODO:CODE

热身期可以应用于任何调度器(不仅仅是cos)。更多关于学习率计划的详细讨论和更多的实验也见[Gotmare等人，2018](http://preview.d2l.ai/d2l-en/PR-1102/chapter_references/zreferences.html#gotmare-keskar-xiong-ea-2018)。特别地，他们发现预热阶段限制了深度网络中参数的发散量。这直观上是有道理的，因为我们预计，由于网络中那些在开始时花费最多时间进行进展的部分的随机初始化，会产生显著的差异。

## 小结

- 训练期间降低学习率可以提高准确性，并（最令人困惑的是）减少模型的过拟合。
- 在实践中，只要进度停滞，学习率的逐段降低是有效的。 本质上，这确保了我们有效地收敛到合适的解决方案，然后才通过降低学习率来减少参数的固有差异。
- 余弦调度程序因某些计算机视觉问题而很流行。 有关此类调度程序的详细信息，请参见例如GluonCV。
- 优化前的预热阶段可以防止发散。
- 优化在深度学习中有多个目的。 除了最大程度地减少训练目标之外，优化算法和学习率调度的不同选择还可能1. 导致测试集的泛化和拟合过度（针对相同数量的训练误差）。

## 练习

1. 对于给定的固定学习率，尝试优化行为。 您可以通过这种方式获得的最佳模型是什么？
1. 如果更改学习率下降的指数，收敛会如何变化？ 使用PolyScheduler为您的实验提供方便。
1. 将余弦调度程序应用于大型计算机视觉问题，例如培训ImageNet。 相对于其他调度程序，它如何影响性能？
1. 预热应持续多长时间？
1. 您可以连接优化和抽样吗？ 首先使用[Welling＆Teh，2011]关于随机梯度Langevin动力学的结果。
