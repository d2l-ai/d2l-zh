

<!--
 * @version:
 * @Author:  StevenJokes https://github.com/StevenJokes
 * @Date: 2020-07-03 20:53:18
 * @LastEditors:  StevenJokes https://github.com/StevenJokes
 * @LastEditTime: 2020-07-03 22:36:45
 * @Description:
 * @TODO::
 * @Reference:
-->
# 凸性
:label:TODO:

凸性在优化算法设计中起着至关重要的作用。这很大程度上是因为在这种环境下分析和测试算法要容易得多。换句话说，如果算法即使在凸集上也表现不佳，我们就不应该希望看到好的结果。此外，尽管深度学习中的优化问题通常是非凸问题，但它们往往在局部极小点附近表现出凸问题的一些性质。这可能导致令人兴奋的新的优化变体，如:cite: [Izmailov et al., 2018](http://preview.d2l.ai/d2l-en/PR-1102/chapter_references/zreferences.html#izmailov-podoprikhin-garipov-ea-2018)。

## 基础

让我们从基础开始。

这听起来有点抽象。考虑[图11.2.1](http://preview.d2l.ai/d2l-en/PR-1102/chapter_optimization/convexity.html#fig-pacman)所示的图片。第一个集合不是凸的，因为它不包含线段。另外两组则没有这样的问题。


为了说明这一点，让我们绘制一些函数，并检查哪些满足需求。我们需要导入一些库。

TODO:CODE

让我们定义几个函数，凸函数和非凸函数。

TODO:CODE

正如预期的那样，余弦函数是非凸的，而抛物线和指数函数是非凸的。注意，要使条件有意义，X 是凸集这一要求是必要的。否则，f(λx+(1−λ)x′) 的结果可能没有很好的定义。凸函数有许多令人满意的性质。

## Jensen不等式

Jensen不等式是最有用的工具之一。它相当于凸性定义的一般化：

TODO:MATH

其中， α_i 是非负的实数，例如 ∑iαi=1  。换句话说，凸函数的期望比期望的凸函数要大。为了证明第一个不等式，我们将凸性的定义反复地应用到和式中的一项上。期望可以通过取有限段的极限来证明。

因为P(y)P(x y)dy=P(x) P(y)P(x y)dy=P(x)这是用在变分方法中。这里yy是典型的未观测随机变量，P(y) 是对其分布的最佳猜测 P(x) 是对 y 积分后的分布。例如聚类中yy可能是聚类标签， P(x，y) 是应用聚类标签的生成模型。

Jensen不等式的一个常见应用是关于部分观测随机变量的对数似然性。也就是说，我们使用

TODO:MATH

因为 ∫P(y)P(x∣y)dy=P(x) 这是用在变分方法中。这里 y 是典型的未观测随机变量， P(y) 是对其分布的最佳猜测 P(x) 是对 y 积分后的分布。例如聚类中yy可能是聚类标签， P(x∣y) 是应用聚类标签的生成模型。

## 性质

凸函数有一些有用的性质。我们这样描述它们。

### 没有局部最小值

特别是，凸函数没有局部极小值。让我们假设相反的情况，并证明它是错的。如果 x∈X 是局部最小值，则存在x的某个邻域，其中 f(x) 是其最小值。由于 x 只是一个局部极小值，所以必须有另一个x′∈X ，使 f(x') < f(x) 。但是，通过凸性，整个直线 在λ∈[0,1) 范围的 λx+(1−λ)x′ 上的函数值必须少于f(x′)，如:

TODO:MATH

这与 f(x) 是局部最小值的假设相矛盾。例如，函数f(x)=(x+1)(x-1)^2在 x=1 时具有局部最小值。然而，它不是一个全局最小值。

TODO:MATH

凸函数没有局部极小值这一事实是非常方便的。这意味着，如果我们最小化函数，就不会陷入困境。但是请注意，这并不意味着不能有一个以上的全局最小值，或者甚至可能存在一个全局最小值。例如，函数 f(x)=max(|x|-1,0)f(x)=max(|x| 1,0) 在区间 [1,1] 内达到其最小值。相反，函数 f(x)=exp(x) 在 R 上没有得到最小值。对于 x ，它趋近于 0 ，但是没有 x 使得 f(x)=0 。

至少近似满足约束优化问题的一种方法是采用拉格朗日函数。与其满足 ci(x) 0 ，我们只是简单地将侧重的c_i(x)≤0侧重的  α_ic_i(x) 添加到目标函数 f(x) 中。这确保了约束不会被严重违反

一般来说，解决一个约束优化问题是困难的。解决这个问题的一种方法来自物理学，有一种相当简单的直觉。想象一个球在一个盒子里。球会滚到最低的地方，重力会被盒子的两边施加在球上的力抵消。简而言之，目标函数(即。将被约束函数的梯度所抵消(由于墙的向后推，需要保持在盒子内)。注意，任何不活动的约束(例如。，球不发出来

跳过拉格朗日函数 L 的推导(详见Boyd和Vandenberghe的书[Boyd & Vandenberghe, 2004](http://preview.d2l.ai/d2l-en/PR-1102/chapter_references/zreferences.html#boyd-vandenberghe-2004))，上述推理可以通过以下鞍点优化问题来表达:

TODO:MATH

这里的变量是所谓的拉格朗日乘数，确保适当执行约束。它们被选择的足够大，以确保所有 i 的 c_i(x)<=0。例如，对于 c_i(x)<0 自然地<0的任何xx，我们最终会选择的是:此外，这是一个鞍点优化问题，在这个问题中，我们希望最大化关于外置的LL，同时最小化关于 x 的 L 。有大量的文献解释如何得到函数 L(x，α) 。为了我们的目的，知道i的鞍点就足够了。

### 惩罚项

实际上，我们一直在用这个技巧。考虑4.5节中的重量衰减。在该方法中，我们在目标函数中加入了新的模型，以确保ww不会变得太大。使用约束优化的观点，我们可以看到，这将确保w2r2 0 w2r2 0为一些半径rr。通过调整 λ 的值，我们可以改变 w 的大小。

通常，添加惩罚是确保近似约束满足的一种好方法。在实践中，这比精确的满足更加可靠。此外，对于非凸问题，许多使精确方法在凸情况下如此吸引人的特性(例如最优性)不再成立。

### 投影

满足约束条件的另一种策略是投影。同样，我们以前遇到过它们，比如在8.5节中处理渐变剪辑时。在这里，我们确保了梯度的长度以 c 为界通过

TODO:MATH

这就是 g 在半径为 c 的球上的投影。更一般地，(凸)集 X上 的投影定义为

TODO:MATH

因此，它是 X 到 x 的最近点。这听起来有点抽象。[图11.2.4](http://preview.d2l.ai/d2l-en/PR-1102/chapter_optimization/convexity.html#fig-projections)更清楚地说明了这一点。其中有两个凸集，一个圆和一个菱形。集合内的点(黄色)保持不变。集合外的点(黑色)映射到集合内最近的点(红色)。而对于 l2 球，这使方向不变，这不必是一般情况下，可以看到菱形的情况。

凸投影的一个用途是计算稀疏权值向量。在这种情况下，我们将 w 投影到一个 l1 的球上(后者是上图中钻石的广义版本)。

## 小结

在深度学习的背景下，凸函数的主要目的是激励优化算法，并帮助我们了解它们的细节。下面我们将看到如何相应地推导梯度下降和随机梯度下降。

* 凸集的交集是凸的。 并集不是。
* 凸函数的期望大于期望的凸函数（詹森不等式）。
* 当且仅当其二阶导数在整个过程中仅具有非负特征值时，二次可微函数才是凸函数。
* 可以通过Lagrange函数添加凸约束。 实际上，只需将它们加到目标函数中即可。
* 投影映射到（凸）集中最接近原始点的点。
