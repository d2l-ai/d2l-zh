# 区域卷积神经网络（R-CNN）系列

除了 :numref:`sec_ssd` 中描述的单发多框检测之外，
基于区域的 CNN （region-based CNN，R-CNN）:cite:`Girshick.Donahue.Darrell.ea.2014` 也是将深度模型应用于目标检测的开创性工作之一 。
在本节中，我们将介绍 R-CNN 及其一系列改进：Fast R-CNN :cite:`Girshick.2015`、Faster R-CNN :cite:`Ren.He.Girshick.ea.2015` 和 Mask R-CNN :cite:`He.Gkioxari.Dollar.ea.2017`。
限于篇幅，这里只介绍这些模型的设计思路。 

## R-CNN

*R-CNN* 首先从输入图像中提取许多（例如，2000）* 地区提案 *（例如，锚框也可以被视为区域提案），标记其类别和边界框（例如抵消）。:cite:`Girshick.Donahue.Darrell.ea.2014` 然后，CNN 用于对每个地区提案进行向前传播以提取其功能。接下来，每个区域提案的特征都用于预测该区域提案的类别和边界框。 

![The R-CNN model.](../img/r-cnn.svg)
:label:`fig_r-cnn`

:numref:`fig_r-cnn` 显示了 R-CNN 模型。更具体地说，R-CNN 包括以下四个步骤： 

1. 执行 * 选择性搜索 * 以在输入图像 :cite:`Uijlings.Van-De-Sande.Gevers.ea.2013` 上提取多个高质量的区域提案。这些建议的区域通常以不同形状和大小的多个比例进行选择。每个地区提案都将标有一个课堂和一个地面真实边界框。
1. 选择预训练有线电视新闻网络，然后在输出图层之前将其截断。将每个区域方案的大小调整为网络所需的输入大小，然后通过向前传播输出为区域提案提取的要素。 
1. 以每个区域提案的提取的要素和标记类为例。训练多个支持向量机来对对象进行分类，其中每个支持向量机单独确定示例是否包含特定类。
1. 以每个区域提案的提取的要素和标记的边界框为例。训练线性回归模型来预测地面真实边界框。

尽管 R-CNN 模型使用预训练的 CNN 来有效提取图像特征，但速度很慢。想象一下，我们从单个输入图像中选择了数千个区域提案：这需要数千个 CNN 正向传播来执行对象检测。这种庞大的计算负载使得在现实世界的应用中广泛使用 R-CNN 是不可行的。 

## 快速 R-CNN

R-CNN 的主要性能瓶颈在于每个地区提案的独立 CNN 向前传播，而没有共享计算。由于这些区域通常有重叠，因此独立的要素提取会导致重复进行计算。R-CNN 的 * 快速 R-CNN* 的主要改进之一是 CNN 仅在整个映像 :cite:`Girshick.2015` 上执行 CNN 的前向传播。  

![The fast R-CNN model.](../img/fast-rcnn.svg)
:label:`fig_fast_r-cnn`

:numref:`fig_fast_r-cnn` 描述了快速的 R-CNN 模型。它的主要计算如下： 

1. 与 R-CNN 相比，在快速的 R-CNN 中，CNN 对特征提取的输入是整个图像，而不是单个区域的提案。此外，这个有线电视新闻网是可培训的。给定输入图像，让 CNN 输出的形状为 $1 \times c \times h_1  \times w_1$。
1. 假设选择性搜索产生了 $n$ 个区域提案。这些地区提案（形状不同）在有线电视新闻网的输出中标记了感兴趣的区域（形状不同）。然后，这些感兴趣的区域进一步提取相同形状的特征（比如指定高度 $h_2$ 和宽度 $w_2$），以便轻松连接。为了实现这一目标，快速 R-CNN 引入了 * 感兴趣区域 (ROI) 池 * 层：CNN 的输出和区域提案被输入到该层中，输出形状 $n \times c \times h_2 \times w_2$ 的连接要素，这些要素将进一步提取到所有区域提案。
1. 使用完全连接的图层，将连接的要素转换为形状 $n \times d$ 的输出，其中 $d$ 取决于模型设计。
1. 预测 $n$ 区域提案中每个提案的类别和边界框。更具体地说，在类和边界框预测中，将完全连接的层输出转换为形状 $n \times q$（$q$ 是类数）的输出和形状 $n \times 4$ 的输出。班级预测使用 softmax 回归。

快速 R-CNN 中提出的感兴趣区域集中层与 :numref:`sec_pooling` 中引入的池层不同。在池层中，我们通过指定池窗口、填充和步幅的大小来间接控制输出形状。相比之下，我们可以直接在感兴趣区域集中层中指定输出形状。 

例如，让我们将每个区域的输出高度和宽度分别指定为 $h_2$ 和 $w_2$。对于任何形状为 $h \times w$ 的感兴趣区域窗口，该窗口被划分为 $h_2 \times w_2$ 网格的子窗口，其中每个子窗口的形状约为 $(h/h_2) \times (w/w_2)$。实际上，任何子窗口的高度和宽度都应向上舍入，最大的元素应用作子窗口的输出。因此，感兴趣区域池图层可以提取相同形状的要素，即使感兴趣的区域具有不同的形状。 

作为说明性示例，在 :numref:`fig_roi` 中，在 $4 \times 4$ 输入中选择了左上角 $3\times 3$ 感兴趣区域。对于这个感兴趣的区域，我们使用 $2\times 2$ 感兴趣区域聚集层来获得 $2\times 2$ 输出。请注意，四个分割的子窗口中的每个都包含元素 0、1、4 和 5（5 是最大值）；2 和 6（最大值为 6）；8 和 9（最大值为 9）和 10。 

![A $2\times 2$ region of interest pooling layer.](../img/roi.svg)
:label:`fig_roi`

下面我们演示了感兴趣区域池层的计算方法。假设 CNN 提取的功能 `X` 的高度和宽度都是 4，而且只有一个通道。

```{.python .input}
from mxnet import np, npx

npx.set_np()

X = np.arange(16).reshape(1, 1, 4, 4)
X
```

```{.python .input}
#@tab pytorch
import torch
import torchvision

X = torch.arange(16.).reshape(1, 1, 4, 4)
X
```

让我们进一步假设输入图像的高度和宽度都是 40 像素，选择性搜索会在此图像上生成两个区域提案。每个区域提案都表示为五个元素：其对象类，后跟其左上角和右下角的 $(x, y)$ 坐标。

```{.python .input}
rois = np.array([[0, 0, 0, 20, 20], [0, 0, 10, 30, 30]])
```

```{.python .input}
#@tab pytorch
rois = torch.Tensor([[0, 0, 0, 20, 20], [0, 0, 10, 30, 30]])
```

由于 `X` 的高度和宽度是输入图像高度和宽度的 $1/10$，因此根据指定的 `spatial_scale` 参数，两个区域提案的坐标乘以 0.1。然后，这两个感兴趣的地区分别在 `X` 和 `X[:, :, 1:4, 0:4]` 上标记为 `X[:, :, 1:4, 0:4]`。最后，在 $2\times 2$ 感兴趣区域集中，每个感兴趣区域被划分为一个子窗口网格，以进一步提取相同形状 $2\times 2$ 的特征。

```{.python .input}
npx.roi_pooling(X, rois, pooled_size=(2, 2), spatial_scale=0.1)
```

```{.python .input}
#@tab pytorch
torchvision.ops.roi_pool(X, rois, output_size=(2, 2), spatial_scale=0.1)
```

## 更快的 R-CNN

为了更准确地检测目标，快速 R-CNN 模型通常必须在选择性搜索中生成大量区域提案。为了在不损失准确性的情况下减少地区提案，* 速度 R-CNN* 建议将选择性搜索替换为 * 区域提案网络 * :cite:`Ren.He.Girshick.ea.2015`。 

![The faster R-CNN model.](../img/faster-rcnn.svg)
:label:`fig_faster_r-cnn`

:numref:`fig_faster_r-cnn` 显示了 R-CNN 模型的速度更快。与快速的 R-CNN 相比，速度更快的 R-CNN 只会将区域提案方法从选择性搜索改为区域提案网络。模型的其余部分保持不变。区域提案网络的工作步骤如下： 

1. 使用填充为 1 的 $3\times 3$ 卷积图层将 CNN 输出转换为具有 $c$ 通道的新输出。通过这种方式，沿 CNN 提取的要素地图的空间维度上的每个单位将获得一个长度为 $c$ 的新要素矢量。
1. 以要素地图的每个像素为中心，生成具有不同比例和长宽比的多个锚点框并对其进行标记。
1. 使用位于每个锚点框中心的长度 $c$ 要素矢量，预测此锚点框的二进制类（背景或对象）和边界框。
1. 考虑那些预测类为对象的预测边界框。使用非最大隐藏删除重叠的结果。对象的剩余预测边界框是感兴趣区域集中层所需的区域提案。

值得注意的是，作为更快的 R-CNN 模型的一部分，区域提案网络与模型的其他部分联合进行了培训。换句话说，更快的 R-CNN 的客观功能不仅包括目标检测中的类和边界框预测，还包括区域建议书网络中锚框的二进制类和边界框预测。作为端到端培训的结果，区域计划书网络学习了如何生成高质量的区域建议书，以便在减少从数据中学习的区域提案的数量的情况下保持目标检测的准确性。 

## 面具 R-CNN

在训练数据集中，如果图像上也标记物体的像素级位置，*mask R-CNN* 可以有效地利用这些详细的标签来进一步提高物体检测 :cite:`He.Gkioxari.Dollar.ea.2017` 的准确性。 

![The mask R-CNN model.](../img/mask-rcnn.svg)
:label:`fig_mask_r-cnn`

如 :numref:`fig_mask_r-cnn` 所示，掩码 R-CNN 是基于更快的 R-CNN 进行修改的。具体来说，掩码 R-CNN 将感兴趣的区域池层替换为
*感兴趣区域 (ROI) 对齐 * 层。 
此感兴趣区域对齐图层使用双线性插值法来保留要素地图上的空间信息，这更适合像素级别的预测。此图层的输出包含所有感兴趣区域的相同形状的要素地图。它们不仅用于预测每个感兴趣区域的类和边界框，还可以通过额外的完全卷积网络预测物体的像素级位置。本章的后续章节将提供有关使用完全卷积网络预测图像像素级语义的更多详细信息。 

## 小结

* R-CNN 从输入图像中提取许多区域提案，使用 CNN 对每个区域提案执行前向传播以提取其特征，然后使用这些功能来预测该区域提案的类别和边界框。
* 来自 R-CNN 的快速 R-CNN 的主要改进之一是 CNN 的前向传播仅在整个图像上执行。它还引入了感兴趣区域池图层，以便可以为具有不同形状的感兴趣区域进一步提取相同形状的要素。
* R-CNN 的速度更快，将快速 R-CNN 中使用的选择性搜索替换为联合训练的区域提案网络，这样前者可以通过减少区域提案来保持目标检测的准确性。
* 基于速度更快的 R-CNN，掩码 R-CNN 还引入了一个完全卷积网络，以便利用像素级标签进一步提高物体检测的准确性。

## 练习

1. 我们能否将物体检测作为单个回归问题，例如预测边界框和类概率？你可以参考 YOLO 型号 :cite:`Redmon.Divvala.Girshick.ea.2016` 的设计。
1. 将单次多盒检测与本节介绍的方法进行比较。他们的主要区别是什么？你可以参考 :cite:`Zhao.Zheng.Xu.ea.2019` 中的图 2。

:begin_tab:`mxnet`
[Discussions](https://discuss.d2l.ai/t/3206)
:end_tab:

:begin_tab:`pytorch`
[Discussions](https://discuss.d2l.ai/t/3207)
:end_tab:
