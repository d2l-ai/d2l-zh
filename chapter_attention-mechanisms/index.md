# 注意机制
:label:`chap_attention`

灵长类动物视觉系统的视神经接受大量的感官输入，远远超过了大脑能够完全处理的程度。幸运的是，并非所有的刺激都是平等的。意识的聚集和集中使灵长类动物能够在复杂的视觉环境中将注意力引向感兴趣的物体，例如猎物和掠食动物。只关注一小部分信息的能力具有进化意义，使人类能够生存和成功。

自 19 世纪以来，科学家们一直在研究认知神经科学领域的注意力。在本章中，我们将首先回顾一个热门框架，解释如何在视觉场景中部署注意力。受此框架中的注意线索的启发，我们将设计利用这些关注线索的模型。值得注意的是，1964 年的 Nadaraya-Waston 内核回归是具有 * 注意力机制 * 的机器学习的简单演示。

接下来，我们将继续介绍在深度学习中注意力模型设计中广泛使用的注意力函数。具体来说，我们将展示如何使用这些函数来设计 *Bahdanau 注意力 *，这是深度学习中的突破性注意力模型，可以双向对齐并且可以区分。

最后，配备了最近的
*多头关注 *
和 * 自我关注 * 设计，我们将仅基于注意机制来描述 *Transer* 架构。自 2017 年提出建议以来，变形金刚一直在现代深度学习应用中普遍存在，例如语言、视觉、语音和强化学习领域。

```toc
:maxdepth: 2

attention-cues
nadaraya-waston
attention-scoring-functions
bahdanau-attention
multihead-attention
self-attention-and-positional-encoding
transformer
```
